<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>the VALUE team | Santander Meteorology Group</title><link>https://santandermetgroup.github.io/website-demo/author/the-value-team/</link><atom:link href="https://santandermetgroup.github.io/website-demo/author/the-value-team/index.xml" rel="self" type="application/rss+xml"/><description>the VALUE team</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 01 Jan 1970 00:33:36 +0000</lastBuildDate><image><url>https://santandermetgroup.github.io/website-demo/media/icon_hu58a305e0ce1458c8ebaf599b64640cde_12944_512x512_fill_lanczos_center_3.png</url><title>the VALUE team</title><link>https://santandermetgroup.github.io/website-demo/author/the-value-team/</link></image><item><title>An intercomparison of a large ensemble of statistical downscaling methods for Europe: Overall results from the VALUE perfect predictor cross-validation experiment</title><link>https://santandermetgroup.github.io/website-demo/event/2016-an-intercomparison-of-a-large/</link><pubDate>Thu, 01 Jan 1970 00:33:36 +0000</pubDate><guid>https://santandermetgroup.github.io/website-demo/event/2016-an-intercomparison-of-a-large/</guid><description>&lt;p>VALUE is an open European network to validate and compare downscaling methods for climate change research (&lt;a href="http://www.value-cost.eu" title="http://www.value-cost.eu">http://www.value-cost.eu&lt;/a>). A key deliverable of VALUE is the development of a systematic validation framework to enable the assessment and comparison of both dynamical and statistical downscaling methods. This framework is based on a user-focused validation tree, guiding the selection of relevant validation indices and performance measures for different aspects of the validation (marginal, temporal, spatial, multi-variable). Moreover, several experiments have been designed to isolate specific points in the downscaling procedure where problems may occur (assessment of intrinsic performance, effect of errors inherited from the global models, effect of non-stationarity, etc.). The list of downscaling experiments includes 1) cross-validation with perfect predictors, 2) GCM predictors –aligned with EURO-CORDEX experiment– and 3) pseudo reality predictors (see Maraun et al. 2015, Earth’s Future, 3, doi:10.1002/2014EF000259, for more details). The results of these experiments are gathered, validated and publicly distributed through the VALUE validation portal, allowing for a comprehensive community-open downscaling intercomparison study.&lt;/p>
&lt;p>In this contribution we describe the overall results from Experiment 1), consisting of a European wide 5- fold cross-validation (with consecutive 6-year periods from 1979 to 2008) using predictors from ERA-Interim to downscale precipitation and temperatures (minimum and maximum) over a set of 86 ECA&amp;amp;D stations representative of the main geographical and climatic regions in Europe. As a result of the open call for contribution to this experiment (closed in Dec. 2015), over 40 methods representative of the main approaches (MOS and Perfect Prognosis, PP) and techniques (linear scaling, quantile mapping, analogs, weather typing, linear and generalized regression, weather generators, etc.) were submitted, including information both data (downscaled values) and metadata (characterizing different aspects of the downscaling methods). This constitutes the largest and most comprehensive to date intercomparison of statistical downscaling methods. Here, we present an overall validation, analyzing marginal and temporal aspects to assess the intrinsic performance and added value of statistical downscaling methods at both annual and seasonal levels. This validation takes into account the different properties/limitations of different approaches and techniques (as reported in the provided metadata) in order to perform a fair comparison. It is pointed out that this experiment alone is not sufficient to evaluate the limitations of (MOS) bias correction techniques. Moreover, it also does not fully validate PP since we don’t learn whether we have the right predictors and whether the PP assumption is valid. These problems will be analyzed in the subsequent community-open VALUE experiments 2) and 3), which will be open for participation along the present year.&lt;/p></description></item></channel></rss>